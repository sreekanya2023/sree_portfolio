group_by(Date) %>%
summarise(Q1 = quantile(Weekly_Sales, 0.25),
Q3 = quantile(Weekly_Sales, 0.75))
ggplot(quartile_data, aes(x = Date)) +
geom_line(aes(y = Q1), color = "#673AB7", linetype = "dashed") +
geom_line(aes(y = Q3), color = "#673AB7", linetype = "dashed") +
labs(title = "Quartile Sales Over Time",
x = "Date", y = "Weekly Sales",
color = "Quartile") +
theme_minimal()
})
# Reactive expression for line plot - Weekly Sales Over Time
output$weekly_sales_over_time_plot <- renderPlot({
ggplot(Walmart, aes(x = Date, y = Weekly_Sales, color = as.factor(Store))) +
geom_line() +
labs(title = "Weekly Sales Over Time",
x = "Date", y = "Weekly Sales") +
theme_minimal()
})
# Reactive expression for average weekly sales by store plot with error bars
output$avg_sales_by_store_plot <- renderPlotly({
ggplotly(
ggplot(average_sales_by_store, aes(x = as.factor(Store), y = Avg_Weekly_Sales)) +
geom_bar(stat = "identity", fill = "#FF4081", alpha = 0.7) +
geom_errorbar(aes(ymin = Avg_Weekly_Sales - Std_Dev, ymax = Avg_Weekly_Sales + Std_Dev),
width = 0.2, color = "#E91E63") +
labs(title = "Average Weekly Sales by Store with Error Bars",
x = "Store", y = "Average Weekly Sales") +
theme_minimal()
)
})
# Reactive expression for average weekly sales by month plot with error bars
output$avg_sales_by_month_plot <- renderPlotly({
ggplotly(
ggplot(average_sales_by_month, aes(x = as.factor(Month), y = Avg_Weekly_Sales)) +
geom_bar(stat = "identity", fill = "#42A5F5", alpha = 0.7) +
geom_errorbar(aes(ymin = Avg_Weekly_Sales - Std_Dev, ymax = Avg_Weekly_Sales + Std_Dev),
width = 0.2, color = "#0D47A1") +
labs(title = "Average Weekly Sales by Month with Error Bars",
x = "Month", y = "Average Weekly Sales") +
theme_minimal()
)
})
# Reactive expression for combined store and date average weekly sales line graph with error bars
output$combined_sales_line_graph <- renderPlotly({
ggplotly(
ggplot(average_sales_by_store_and_date, aes(x = Date, y = Avg_Weekly_Sales, color = as.factor(Store))) +
geom_line() +
geom_errorbar(aes(ymin = Avg_Weekly_Sales - Std_Dev, ymax = Avg_Weekly_Sales + Std_Dev),
width = 0.2, color = "#E91E63") +
labs(title = "Combined Store and Date Average Weekly Sales Line Graph with Error Bars",
x = "Date", y = "Avg Weekly Sales", color = "Store") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
)
})
# Reactive expression for correlation heatmap
output$correlation_heatmap <- renderPrint({
# Select only numeric columns for correlation calculation
numeric_columns <- sapply(Walmart, is.numeric)
numeric_data <- Walmart[, numeric_columns]
# Calculate correlation matrix
correlation_matrix <- cor(numeric_data, use = "complete.obs")
# Print correlation matrix values
print(correlation_matrix)
})
# Additional reactive expression for rendering the heatmap plot
output$correlation_heatmap_plot <- renderPlot({
# Select only numeric columns for correlation calculation
numeric_columns <- sapply(Walmart, is.numeric)
numeric_data <- Walmart[, numeric_columns]
# Calculate correlation matrix
correlation_matrix <- cor(numeric_data, use = "complete.obs")
# Create a heatmap with annotations
heatmap.2(
correlation_matrix,
main = "Correlation Heatmap",
col = colorRampPalette(c("#1976D2", "white", "#FFA000"))(20),
scale = "none",
margins = c(10, 10),
xlab = "Variables",
ylab = "Variables",
cexRow = 1, cexCol = 1,  # Adjust text size
key.title = title("Correlation", cex.main = 1),  # Adjust key title size
notecol = "black",  # Color of the correlation values
tracecol = NULL,    # Color of the trace lines
density.info = "none",  # Do not display density plot
key = TRUE,          # Display color key
keysize = 1.5,       # Size of the key
symkey = FALSE       # Do not use symmetric key
)
})
# Reactive expression for model evaluation metrics
output$model_evaluation <- renderPrint({
# Split the data into training and testing sets
set.seed(123)  # Set seed for reproducibility
train_indices <- createDataPartition(Walmart$Weekly_Sales, p = 0.7, list = FALSE)
trainData <- Walmart[train_indices, ]
testData <- Walmart[-train_indices, ]
# Evaluate the linear regression model
lm_model <- lm(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
lm_predictions <- predict(lm_model, newdata = testData)
lm_r2 <- cor(lm_predictions, testData$Weekly_Sales)^2
lm_rmse <- sqrt(mean((lm_predictions - testData$Weekly_Sales)^2))
# Evaluate the random forest model
rf_model <- randomForest(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
rf_predictions <- predict(rf_model, newdata = testData)
rf_r2 <- cor(rf_predictions, testData$Weekly_Sales)^2
rf_rmse <- sqrt(mean((rf_predictions - testData$Weekly_Sales)^2))
cat("Linear Regression Model:\n")
cat("R-squared:", lm_r2, "\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("R-squared:", rf_r2, "\n")
cat("RMSE:", rf_rmse, "\n")
})
}
# Run the application
shinyApp(ui, server)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(plotly)
library(gplots)
library(caret)
library(randomForest)
library(e1071)
# Load the Walmart data
Walmart <- read.csv("C:/Users/HP/Downloads/Walmart.csv")
# Data Cleanup
# Convert Date column to Date format
Walmart$Date <- as.Date(Walmart$Date)
# Feature Engineering
# Extract Year, Month, and Day from Date
Walmart$Year <- format(Walmart$Date, "%Y")
Walmart$Month <- format(Walmart$Date, "%m")
Walmart$Day <- format(Walmart$Date, "%d")
# Exploratory Data Analysis (EDA)
# Display the first few rows of the dataset
head(Walmart)
# Summary statistics
summary(Walmart)
# Distribution plot for Weekly Sales
ggplot(Walmart, aes(x = Weekly_Sales)) +
geom_histogram(binwidth = 5000, fill = "#4CAF50", color = "#2E7D32", alpha = 0.7) +
labs(title = "Distribution of Weekly Sales",
x = "Weekly Sales", y = "Frequency") +
theme_minimal()
# Boxplot for Weekly Sales by Store
ggplot(Walmart, aes(x = Store, y = Weekly_Sales)) +
geom_boxplot(fill = "#FF4081", alpha = 0.7) +
labs(title = "Boxplot of Weekly Sales by Store",
x = "Store", y = "Weekly Sales") +
theme_minimal()
# Scatter plot for Weekly Sales and Temperature
ggplot(Walmart, aes(x = Temperature, y = Weekly_Sales)) +
geom_point(color = "#1976D2", alpha = 0.7) +
labs(title = "Scatter Plot of Weekly Sales vs Temperature",
x = "Temperature", y = "Weekly Sales") +
theme_minimal()
# Quartile plot
quartile_data <- Walmart %>%
group_by(Date) %>%
summarise(Q1 = quantile(Weekly_Sales, 0.25),
Q3 = quantile(Weekly_Sales, 0.75))
ggplot(quartile_data, aes(x = Date)) +
geom_line(aes(y = Q1), color = "#673AB7", linetype = "dashed") +
geom_line(aes(y = Q3), color = "#673AB7", linetype = "dashed") +
labs(title = "Quartile Sales Over Time",
x = "Date", y = "Weekly Sales",
color = "Quartile") +
theme_minimal()
# Model Evaluation
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(Walmart$Weekly_Sales, p = 0.7, list = FALSE)
trainData <- Walmart[train_indices, ]
testData <- Walmart[-train_indices, ]
# Evaluate the linear regression model
lm_model <- lm(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
lm_predictions <- predict(lm_model, newdata = testData)
lm_r2 <- cor(lm_predictions, testData$Weekly_Sales)^2
lm_rmse <- sqrt(mean((lm_predictions - testData$Weekly_Sales)^2))
# Evaluate the random forest model
rf_model <- randomForest(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
rf_predictions <- predict(rf_model, newdata = testData)
rf_r2 <- cor(rf_predictions, testData$Weekly_Sales)^2
rf_rmse <- sqrt(mean((rf_predictions - testData$Weekly_Sales)^2))
cat("Linear Regression Model:\n")
cat("R-squared:", lm_r2, "\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("R-squared:", rf_r2, "\n")
cat("RMSE:", rf_rmse, "\n")
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(plotly)
library(gplots)
library(caret)
library(randomForest)
library(e1071)
# Load the Walmart data
Walmart <- read.csv("C:/Users/HP/Downloads/Walmart.csv")
# Data Cleanup
# Convert Date column to Date format
Walmart$Date <- as.Date(Walmart$Date)
# Feature Engineering
# Extract Year, Month, and Day from Date
Walmart$Year <- format(Walmart$Date, "%Y")
Walmart$Month <- format(Walmart$Date, "%m")
Walmart$Day <- format(Walmart$Date, "%d")
# Exploratory Data Analysis (EDA)
# Display the first few rows of the dataset
head(Walmart)
# Summary statistics
summary(Walmart)
# Distribution plot for Weekly Sales
ggplot(Walmart, aes(x = Weekly_Sales)) +
geom_histogram(binwidth = 5000, fill = "#4CAF50", color = "#2E7D32", alpha = 0.7) +
labs(title = "Distribution of Weekly Sales",
x = "Weekly Sales", y = "Frequency") +
theme_minimal()
# Boxplot for Weekly Sales by Store
ggplot(Walmart, aes(x = Store, y = Weekly_Sales)) +
geom_boxplot(fill = "#FF4081", alpha = 0.7) +
labs(title = "Boxplot of Weekly Sales by Store",
x = "Store", y = "Weekly Sales") +
theme_minimal()
# Scatter plot for Weekly Sales and Temperature
ggplot(Walmart, aes(x = Temperature, y = Weekly_Sales)) +
geom_point(color = "#1976D2", alpha = 0.7) +
labs(title = "Scatter Plot of Weekly Sales vs Temperature",
x = "Temperature", y = "Weekly Sales") +
theme_minimal()
# Quartile plot
quartile_data <- Walmart %>%
group_by(Date) %>%
summarise(Q1 = quantile(Weekly_Sales, 0.25),
Q3 = quantile(Weekly_Sales, 0.75))
ggplot(quartile_data, aes(x = Date)) +
geom_line(aes(y = Q1), color = "#673AB7", linetype = "dashed") +
geom_line(aes(y = Q3), color = "#673AB7", linetype = "dashed") +
labs(title = "Quartile Sales Over Time",
x = "Date", y = "Weekly Sales",
color = "Quartile") +
theme_minimal()
# Model Evaluation
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(Walmart$Weekly_Sales, p = 0.7, list = FALSE)
trainData <- Walmart[train_indices, ]
testData <- Walmart[-train_indices, ]
# Evaluate the linear regression model
lm_model <- lm(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
lm_predictions <- predict(lm_model, newdata = testData)
lm_r2 <- cor(lm_predictions, testData$Weekly_Sales)^2
lm_rmse <- sqrt(mean((lm_predictions - testData$Weekly_Sales)^2))
# Evaluate the random forest model
rf_model <- randomForest(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
rf_predictions <- predict(rf_model, newdata = testData)
rf_r2 <- cor(rf_predictions, testData$Weekly_Sales)^2
rf_rmse <- sqrt(mean((rf_predictions - testData$Weekly_Sales)^2))
cat("Linear Regression Model:\n")
cat("R-squared:", lm_r2, "\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("R-squared:", rf_r2, "\n")
cat("RMSE:", rf_rmse, "\n")
library(dplyr)
NFLX <- read.csv("C:/Users/HP/Downloads/NFLX.csv", header=TRUE)
library(dplyr)
NFLX <- read.csv("NFLX.csv", header=TRUE)
library(dplyr)
NFLX <- read.csv("NFLX.csv", header=TRUE)
NFLX <- read.csv("D:/QUARTO/SK/Quarto data science portfolio/NFLX.csv")
View(NFLX)
# Load necessary libraries
library(dplyr)
library(ggplot2)
# Load your IRSI data (replace "your_dataset.csv" with your actual dataset file)
IRSI_data <- read.csv("Iris.csv")
Iris <- read.csv("D:/QUARTO/SK/Quarto data science portfolio/Iris.csv")
View(Iris)
# Load necessary libraries
library(dplyr)
library(ggplot2)
# Load your IRSI data (replace "your_dataset.csv" with your actual dataset file)
IRSI_data <- read.csv("D:/QUARTO/SK/Quarto data science portfolio/Iris.csv")
# Display the first few rows of the dataset
head(IRSI_data)
# Check the structure of the dataset
str(IRSI_data)
# Summary statistics for numerical variables
summary(IRSI_data)
# Check for missing values
sapply(IRSI_data, function(x) sum(is.na(x)))
# Frequency table for the Species variable
table(IRSI_data$Species)
# Print column names
colnames(IRSI_data)
# Correlation matrix
cor(IRSI_data[, 1:4])
# Load the corrplot library
library(corrplot)
# Extract the correlation matrix
cor_matrix <- cor(IRSI_data[, 2:5])  # Assuming IRSI_data is your data frame
# Create a correlation plot
corrplot(cor_matrix, method = "circle", tl.col = "black", addrect = 2)
# Boxplot by Species
par(mfrow = c(1, 4))
for (i in 1:4) {
boxplot(IRSI_data[, i] ~ IRSI_data$Species, main = colnames(IRSI_data)[i], col = c("skyblue", "lightgreen", "lightcoral"))
}
# Histogram for SepalLengthCm
hist(IRSI_data$SepalLengthCm, col = "skyblue", main = "Sepal Length Distribution")
# Density plot for PetalWidthCm
ggplot(IRSI_data, aes(x = PetalWidthCm, fill = Species)) +
geom_density(alpha = 0.7) +
ggtitle("Petal Width Density by Species")
# Violin plot for PetalLengthCm
ggplot(IRSI_data, aes(x = Species, y = PetalLengthCm, fill = Species)) +
geom_violin() +
ggtitle("Petal Length Distribution by Species")
library(dplyr)
NFLX <- read.csv("D:/QUARTO/SK/Quarto data science portfolio/NFLX.csv")
names(NFLX)
head(NFLX,n=1)
# Load required libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(kableExtra)
# Read CSV data
NFLX <- read.csv("D:/QUARTO/SK/Quarto data science portfolio/NFLX.csv")
# Calculate missing values percentage function
calculate_missingness <- function(data) {
data %>%
summarise_all(~mean(is.na(.)) * 100)
}
# Calculate missing values
NFLX_missingness <- calculate_missingness(NFLX)
# Print missing values summary
print(NFLX_missingness)
# Visualize missing values
missingness_plot <- function(data) {
data_long <- gather(data, key = "Variable", value = "MissingPercentage")
ggplot(data_long, aes(x = reorder(Variable, -MissingPercentage), y = MissingPercentage)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Missing Values Percentage by Variable",
x = "Variable", y = "Missing Percentage") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
# Generate and display missing values plot
missingness_plot(NFLX_missingness)
# Copy NFLX to NFLX1 without assigning data types
NFLX1 <- NFLX
# Drop rows from the data set when a variable has a missing value
NFLX1 <- NFLX1 %>% na.omit()
# Drop rows from the data set where FPI=6
NFLX1 <- NFLX1 %>% filter(FPI != 6)
# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS
NFLX1 <- NFLX1 %>% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)
# Create a new column named YEAR that is an exact copy of the data in FPEDATS
NFLX1 <- NFLX1 %>% mutate(YEAR = FPEDATS)
# Print out data structure and the summary of NFLX1
str(NFLX1)
summary(NFLX1)
# Create a new column named YEAR that captures the year in FPEDATS in the specified format
NFLX1 <- NFLX1 %>%
mutate(YEAR = format(FPEDATS))
# Calculate the total number of unique analysts providing forecasts each year
NumberAnalyst <- NFLX1 %>%
group_by(YEAR) %>%
distinct(ANALYS) %>%
summarise(NumAnalysts = n_distinct(ANALYS))
# Print the NumberAnalyst object
print(NumberAnalyst)
# Calculate the total number of unique brokerage houses providing forecasts each year
NumberBrokerage <- NFLX1 %>%
group_by(YEAR) %>%
distinct(ESTIMATOR) %>%
summarise(NumBrokerage = n_distinct(ESTIMATOR))
# Print the NumberBrokerage object
print(NumberBrokerage)
# Enter your code for Task 3 below
# Get the most recent forecast for each analyst in each year
NFLX2 <- NFLX1 %>%
group_by(ANALYS, YEAR) %>%
filter(REVDATS == max(REVDATS)) %>%
ungroup()
# Print the dimension of NFLX2
print(dim(NFLX2))
# Check your work
# If NFLX2 has 641 rows and 14 columns, you are on the right track.
# If not, please seek help!
# Create a copy of NFLX2 and call it NFLX3
NFLX3 <- NFLX2
# Task 4: Calculate past accuracy
# For every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy.
NFLX3 <- NFLX3 %>%
group_by(YEAR, ANALYS) %>%
mutate(accuracy = VALUE - ACTUAL)
# For each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy
NFLX3 <- NFLX3 %>%
group_by(ANALYS) %>%
arrange(YEAR) %>%
mutate(past_accuracy = lag(accuracy))
# Check if the code produces 144 NAs
sum(is.na(NFLX3$past_accuracy))
# Enter your code for Task 5 below
# Calculate forecast horizon
NFLX3 <- NFLX3 %>%
mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = "days")))
# Calculate correlation between accuracy and horizon
correlation_by_year <- NFLX3 %>%
group_by(YEAR) %>%
summarise(correlation = cor(accuracy, horizon, use = "complete.obs"))
# Convert YEAR to POSIXlt format
correlation_by_year$YEAR <- as.POSIXlt(correlation_by_year$YEAR, format = "%Y")
correlation_by_year$YEAR <- format(correlation_by_year$YEAR, "%Y-%m-%d %H:%M:%S")
# Find positive correlation year
positive_corr_year <- correlation_by_year %>%
filter(correlation > 0)
# Print positive correlation year with correlation values
print(positive_corr_year)
# If you want to print only the YEAR values without correlation values
# print(positive_corr_year$YEAR)
# Enter your code for Task 6 below
# Calculate cumulative experience
NFLX3 <- NFLX3 %>%
group_by(ANALYS) %>%
mutate(experience = cumsum(!duplicated(YEAR)))
# Find max experience analysts
max_experience <- NFLX3 %>%
group_by(ANALYS) %>%
summarise(experience = max(experience)) %>%
filter(experience == max(experience))
# Summary of experience column
summary(NFLX3$experience)
# Analyst(s) with highest experience
max_experience
# Enter your code for Task 7 below
# Count unique analysts per year and brokerage house (ESTIMATOR)
NFLX3 <- NFLX3 %>%
group_by(YEAR, ESTIMATOR) %>%
mutate(size = n_distinct(ANALYS))
# Print the frequencies for the size variable
size_freq <- table(NFLX3$size)
print(size_freq)
# Create a frequency table for better visualization
size_table <- as.data.frame(size_freq)
colnames(size_table) <- c("Number of Analysts", "Frequency")
# Sort the table by frequency in descending order
size_table <- size_table[order(-size_table$Frequency), ]
# Print the sorted frequency table
print(size_table)
# Summary statistics for size variable
summary(NFLX3$size)
# Assuming you want a colorful plot, using a different color palette
ggplot(size_table, aes(x = factor(`Number of Analysts`), y = Frequency, fill = factor(`Number of Analysts`))) +
geom_bar(stat = "identity") +
ggtitle("Number of Unique Analysts per Year and ESTIMATOR") +
xlab("Number of Analysts") +
ylab("Frequency") +
scale_fill_brewer(palette = "Set3")
# Calculate mean of past_accuracy
mean_pa <- mean(NFLX3$past_accuracy, na.rm = TRUE)
# Create linear regression model
model <- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)
# Get R-squared value
r_squared <- summary(model)$r.squared
# Check R-squared for forecast
if (r_squared > 0.5) {
# Future data
new_data <- data.frame(VALUE = 6.08, past_accuracy = mean_pa)
# Predict EPS for future
pred_eps <- predict(model, newdata = new_data)
# Print forecasted EPS
cat("Forecasted EPS for future period: $", round(pred_eps, 2))
} else {
# Print warning for low R-squared
cat("R-squared is low; model may not predict accurately.")
}
# Print mean past_accuracy
cat("Mean past_accuracy: ", round(mean_pa, 2))
# Calculate mean and median forecasts
mean_forecast <- mean(NFLX3$VALUE, na.rm = TRUE)
median_forecast <- median(NFLX3$VALUE, na.rm = TRUE)
# Print mean and median forecasts
cat("Mean forecast 2020: $", round(mean_forecast, 2))
cat("Median forecast 2020: $", round(median_forecast, 2))
# Aggregate data and calculate yearly averages
NFLX4 <- NFLX3 %>%
group_by(YEAR) %>%
summarise(
size = mean(size, na.rm = TRUE),
experience = mean(experience, na.rm = TRUE),
horizon = mean(horizon, na.rm = TRUE),
accuracy = mean(accuracy, na.rm = TRUE),
past_accuracy = mean(past_accuracy, na.rm = TRUE),
ACTUAL = mean(ACTUAL, na.rm = TRUE)
)
# Summary of NFLX4 dataset
summary(NFLX4)
# Correlation analysis
correlation_matrix <- cor(NFLX4[, c("size", "experience", "horizon", "accuracy", "past_accuracy", "ACTUAL")], use = "complete.obs")
# Print correlation matrix
print(correlation_matrix)
# Create a correlogram
corrplot(correlation_matrix, method = "color", type = "upper")
