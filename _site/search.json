[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sreekanya Peddireddi",
    "section": "",
    "text": "My name is Sreekanya Peddireddi and I’m doing masters in Advanced Data Analytics and This will be my final semester and hoping to complete the course this semester. I completed my bachelor of Science in Computer Science and Engineering and worked in IT for 6 years. The reason I chose this course is that most of my IT experience is related to banking domain and where I deal with large amounts of data, and that is where I realized that I needed to improve my data analytics skills in order to advance in my professional life, and that definitely required some additional learning, so here I am, eager to learn more and grow. I feel that my ambition, hard work, and integrity will propel me to professional success. I also believe in collaboration, which I feel will serve as a stepping stone in my professional career. As an avid enthusiast of a healthy and active lifestyle, I consider exercise an integral part of my daily routine and I like spending my time in gardening and I do play badminton.\nemail:sreekanyapeddireddi@my.unt.edu"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Success is a journey which begins with simple steps, with that this is the first post in my blog. We are going to explore my set of skills as demonmstrated in different areas in consideartions to my understaning in R proghramming language. Welcome!\n\n\nAm a well vast experienced and well trained fellow, the subsequent post are about my portfolio, feel free to dive in.\nThis is the first post in my blog. Welcome!\n\nIn this section blog we will review a few of my projects done on different environments and career period. &gt;&gt;&gt;&gt;&gt;&gt;&gt; c32f1ce5885422cbe2b0d61378a0b0ba0c796fba"
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Surya Vardhan",
    "section": "",
    "text": "Welcome to my corner of the internet! I’m Surya Vardhan, a passionate individual deeply engaged in the world of technology and data. With a curiosity-driven mindset, I explore the realms of data science, machine learning, and beyond. Join me on this journey of discovery and innovation!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Sreekanya Peddireddi",
    "section": "",
    "text": "My name is Sreekanya Peddireddi and I’m doing masters in Advanced Data Analytics and This will be my final semester and hoping to complete the course this semester. I completed my bachelor of Science in Computer Science and Engineering and worked in IT for 6 years. The reason I chose this course is that most of my IT experience is related to banking domain and where I deal with large amounts of data, and that is where I realized that I needed to improve my data analytics skills in order to advance in my professional life, and that definitely required some additional learning, so here I am, eager to learn more and grow. I feel that my ambition, hard work, and integrity will propel me to professional success. I also believe in collaboration, which I feel will serve as a stepping stone in my professional career. As an avid enthusiast of a healthy and active lifestyle, I consider exercise an integral part of my daily routine and I like spending my time in gardening and I do play badminton.\nemail:sreekanyapeddireddi@my.unt.edu"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Comprehensive demonstartion of R prgramming language through a randomly generated dataset.\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplotly\n\n\nplot\n\n\n\n\n\n\n\nSreekanya Peddireddi\n\n\nDec 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData conversion and sorting\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplotly\n\n\nplot\n\n\n\n\n\n\n\nSreekanya Peddireddi\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIris exploratory data analysis\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplotly\n\n\nplot\n\n\n\n\n\n\n\nSreekanya Peddireddi\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nSreekanya Peddireddi\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/postwithplot/index.html",
    "href": "posts/postwithplot/index.html",
    "title": "TidyTuesday: Retail Sales data analysis with Plotly in R",
    "section": "",
    "text": "In this TidyTuesday analysis, I explored retail sales data using R and leveraged the power of Plotly for interactive visualizations. The dataset, likely sourced from the TidyTuesday project, was loaded and examined to gain insights into retail trends. Plotly, a dynamic plotting library, was employed to create engaging and interactive charts, allowing for a deeper understanding of sales patterns, seasonality, or any trends within the retail sector. This approach not only facilitates exploration but also enhances the communicative aspect of data findings through visually appealing and interactive plots.\n\n# Load necessary libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(plotly)\n\nWarning: package 'plotly' was built under R version 4.3.2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(gplots)\n\nWarning: package 'gplots' was built under R version 4.3.2\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.2\n\n\nLoading required package: lattice\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.2\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(e1071)\n\nWarning: package 'e1071' was built under R version 4.3.2\n\n# Load the Walmart data\nWalmart &lt;- read.csv(\"C:/Users/HP/Downloads/Walmart.csv\")\n\n# Data Cleanup\n# Convert Date column to Date format\nWalmart$Date &lt;- as.Date(Walmart$Date)\n\n# Feature Engineering\n# Extract Year, Month, and Day from Date\nWalmart$Year &lt;- format(Walmart$Date, \"%Y\")\nWalmart$Month &lt;- format(Walmart$Date, \"%m\")\nWalmart$Day &lt;- format(Walmart$Date, \"%d\")\n\n# Exploratory Data Analysis (EDA)\n\n# Display the first few rows of the dataset\nhead(Walmart)\n\n  Store       Date Weekly_Sales Holiday_Flag Temperature Fuel_Price      CPI\n1     1 0005-02-20      1643691            0       42.31      2.572 211.0964\n2     1 0012-02-20      1641957            1       38.51      2.548 211.2422\n3     1 0019-02-20      1611968            0       39.93      2.514 211.2891\n4     1 0026-02-20      1409728            0       46.63      2.561 211.3196\n5     1 0005-03-20      1554807            0       46.50      2.625 211.3501\n6     1 0012-03-20      1439542            0       57.79      2.667 211.3806\n  Unemployment Year Month Day\n1        8.106 0005    02  20\n2        8.106 0012    02  20\n3        8.106 0019    02  20\n4        8.106 0026    02  20\n5        8.106 0005    03  20\n6        8.106 0012    03  20\n\n# Summary statistics\nsummary(Walmart)\n\n     Store         Date             Weekly_Sales      Holiday_Flag    \n Min.   : 1   Min.   :0001-04-20   Min.   : 209986   Min.   :0.00000  \n 1st Qu.:12   1st Qu.:0008-07-20   1st Qu.: 553350   1st Qu.:0.00000  \n Median :23   Median :0016-04-20   Median : 960746   Median :0.00000  \n Mean   :23   Mean   :0016-03-07   Mean   :1046965   Mean   :0.06993  \n 3rd Qu.:34   3rd Qu.:0023-12-20   3rd Qu.:1420159   3rd Qu.:0.00000  \n Max.   :45   Max.   :0031-12-20   Max.   :3818686   Max.   :1.00000  \n  Temperature       Fuel_Price         CPI         Unemployment   \n Min.   : -2.06   Min.   :2.472   Min.   :126.1   Min.   : 3.879  \n 1st Qu.: 47.46   1st Qu.:2.933   1st Qu.:131.7   1st Qu.: 6.891  \n Median : 62.67   Median :3.445   Median :182.6   Median : 7.874  \n Mean   : 60.66   Mean   :3.359   Mean   :171.6   Mean   : 7.999  \n 3rd Qu.: 74.94   3rd Qu.:3.735   3rd Qu.:212.7   3rd Qu.: 8.622  \n Max.   :100.14   Max.   :4.468   Max.   :227.2   Max.   :14.313  \n     Year              Month               Day           \n Length:6435        Length:6435        Length:6435       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\n# Distribution plot for Weekly Sales\nggplot(Walmart, aes(x = Weekly_Sales)) +\n  geom_histogram(binwidth = 5000, fill = \"#4CAF50\", color = \"#2E7D32\", alpha = 0.7) +\n  labs(title = \"Distribution of Weekly Sales\",\n       x = \"Weekly Sales\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n# Boxplot for Weekly Sales by Store\nggplot(Walmart, aes(x = Store, y = Weekly_Sales)) +\n  geom_boxplot(fill = \"#FF4081\", alpha = 0.7) +\n  labs(title = \"Boxplot of Weekly Sales by Store\",\n       x = \"Store\", y = \"Weekly Sales\") +\n  theme_minimal()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n# Scatter plot for Weekly Sales and Temperature\nggplot(Walmart, aes(x = Temperature, y = Weekly_Sales)) +\n  geom_point(color = \"#1976D2\", alpha = 0.7) +\n  labs(title = \"Scatter Plot of Weekly Sales vs Temperature\",\n       x = \"Temperature\", y = \"Weekly Sales\") +\n  theme_minimal()\n\n\n\n# Quartile plot\nquartile_data &lt;- Walmart %&gt;%\n  group_by(Date) %&gt;%\n  summarise(Q1 = quantile(Weekly_Sales, 0.25),\n            Q3 = quantile(Weekly_Sales, 0.75))\n\nggplot(quartile_data, aes(x = Date)) +\n  geom_line(aes(y = Q1), color = \"#673AB7\", linetype = \"dashed\") +\n  geom_line(aes(y = Q3), color = \"#673AB7\", linetype = \"dashed\") +\n  labs(title = \"Quartile Sales Over Time\",\n       x = \"Date\", y = \"Weekly Sales\",\n       color = \"Quartile\") +\n  theme_minimal()\n\n\n\n# Model Evaluation\n# Split the data into training and testing sets\nset.seed(123)\ntrain_indices &lt;- createDataPartition(Walmart$Weekly_Sales, p = 0.7, list = FALSE)\ntrainData &lt;- Walmart[train_indices, ]\ntestData &lt;- Walmart[-train_indices, ]\n\n# Evaluate the linear regression model\nlm_model &lt;- lm(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)\nlm_predictions &lt;- predict(lm_model, newdata = testData)\nlm_r2 &lt;- cor(lm_predictions, testData$Weekly_Sales)^2\nlm_rmse &lt;- sqrt(mean((lm_predictions - testData$Weekly_Sales)^2))\n\n# Evaluate the random forest model\nrf_model &lt;- randomForest(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)\nrf_predictions &lt;- predict(rf_model, newdata = testData)\nrf_r2 &lt;- cor(rf_predictions, testData$Weekly_Sales)^2\nrf_rmse &lt;- sqrt(mean((rf_predictions - testData$Weekly_Sales)^2))\n\ncat(\"Linear Regression Model:\\n\")\n\nLinear Regression Model:\n\ncat(\"R-squared:\", lm_r2, \"\\n\")\n\nR-squared: 0.0154542 \n\ncat(\"RMSE:\", lm_rmse, \"\\n\\n\")\n\nRMSE: 553801.7 \n\ncat(\"Random Forest Model:\\n\")\n\nRandom Forest Model:\n\ncat(\"R-squared:\", rf_r2, \"\\n\")\n\nR-squared: 0.140237 \n\ncat(\"RMSE:\", rf_rmse, \"\\n\")\n\nRMSE: 521092.6 \n\n\nThe linear regression model achieved a relatively low R-squared value of 0.0155, indicating that the model explains only a small proportion of the variance in the weekly sales. Additionally, the Root Mean Squared Error (RMSE) of 553,801.7 suggests a significant deviation between the predicted and actual sales values, emphasizing the limitations of the linear model in capturing the underlying patterns in the data.\nOn the other hand, the Random Forest model exhibited better performance with a higher R-squared value of 0.1402, signifying a greater ability to explain variability in weekly sales. The lower RMSE of 521,092.6 further supports the Random Forest model as a more accurate predictor compared to the linear regression model. This suggests that the Random Forest algorithm, which leverages ensemble learning and decision trees, provides a more robust and flexible approach for capturing the complex relationships within the Walmart sales data set."
  },
  {
    "objectID": "projects.html#quarto-blog---data-visualizations---animation-and-interactivity",
    "href": "projects.html#quarto-blog---data-visualizations---animation-and-interactivity",
    "title": "Surya Vadhan",
    "section": "",
    "text": "Description 1\n\n\n\nImage 1"
  },
  {
    "objectID": "projects.html#shiny-flex-dashboard---sales-forecasting-and-anomaly-detection",
    "href": "projects.html#shiny-flex-dashboard---sales-forecasting-and-anomaly-detection",
    "title": "Surya Vadhan",
    "section": "Shiny Flex Dashboard - Sales forecasting and anomaly detection",
    "text": "Shiny Flex Dashboard - Sales forecasting and anomaly detection\n\n\nImage 2\n\n\n\nDescription 2"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/b/index.html",
    "href": "posts/b/index.html",
    "title": "Data conversion and sorting",
    "section": "",
    "text": "# Install and load necessary packages\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.3.2\n\n\nLoading required package: NLP\n\nlibrary(SnowballC)\n\n# Create a sample dataframe with medical content\nmedical_data &lt;- data.frame(\n  PatientID = c(1, 2, 3),\n  Text = c(\n    \"The patient presented with symptoms of persistent cough and shortness of breath. Chest X-ray revealed bilateral infiltrates.\",\n    \"After conducting a thorough examination, the physician diagnosed the patient with hypertension and prescribed antihypertensive medication.\",\n    \"The laboratory results indicated elevated levels of cholesterol and triglycerides. Dietary recommendations were provided to manage the lipid profile.\"\n  ),\n  stringsAsFactors = FALSE\n)\n\n# Create a Corpus (text document) from the Text column\ncorpus &lt;- Corpus(VectorSource(medical_data$Text))\n\n# Perform text preprocessing (removing punctuation, converting to lowercase, stemming)\ncorpus &lt;- tm_map(corpus, content_transformer(tolower))\n\nWarning in tm_map.SimpleCorpus(corpus, content_transformer(tolower)):\ntransformation drops documents\n\ncorpus &lt;- tm_map(corpus, removePunctuation)\n\nWarning in tm_map.SimpleCorpus(corpus, removePunctuation): transformation drops\ndocuments\n\ncorpus &lt;- tm_map(corpus, removeNumbers)\n\nWarning in tm_map.SimpleCorpus(corpus, removeNumbers): transformation drops\ndocuments\n\ncorpus &lt;- tm_map(corpus, removeWords, stopwords(\"en\"))\n\nWarning in tm_map.SimpleCorpus(corpus, removeWords, stopwords(\"en\")):\ntransformation drops documents\n\ncorpus &lt;- tm_map(corpus, stemDocument)\n\nWarning in tm_map.SimpleCorpus(corpus, stemDocument): transformation drops\ndocuments\n\n# Create a Document-Term Matrix (DTM)\ndtm &lt;- DocumentTermMatrix(corpus)\n\n# Convert DTM to a data frame\ndtm_df &lt;- as.data.frame(as.matrix(dtm))\ndtm_df$PatientID &lt;- medical_data$PatientID\n\n# Display the resulting data frame\nprint(dtm_df)\n\n  bilater breath chest cough infiltr patient persist present reveal short\n1       1      1     1     1       1       1       1       1      1     1\n2       0      0     0     0       0       1       0       0      0     0\n3       0      0     0     0       0       0       0       0      0     0\n  symptom xray antihypertens conduct diagnos examin hypertens medic physician\n1       1    1             0       0       0      0         0     0         0\n2       0    0             1       1       1      1         1     1         1\n3       0    0             0       0       0      0         0     0         0\n  prescrib thorough cholesterol dietari elev indic laboratori level lipid manag\n1        0        0           0       0    0     0          0     0     0     0\n2        1        1           0       0    0     0          0     0     0     0\n3        0        0           1       1    1     1          1     1     1     1\n  profil provid recommend result triglycerid PatientID\n1      0      0         0      0           0         1\n2      0      0         0      0           0         2\n3      1      1         1      1           1         3\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::annotate() masks NLP::annotate()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# EDA on the Document-Term Matrix (dtm_df)\n# Summary statistics\nsummary(dtm_df)\n\n    bilater           breath           chest            cough       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.3333   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n    infiltr          patient          persist          present      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.5000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.6667   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:1.0000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     reveal           short           symptom            xray       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.3333   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n antihypertens       conduct          diagnos           examin      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.3333   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n   hypertens          medic          physician         prescrib     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.3333   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n    thorough       cholesterol        dietari            elev       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.3333   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     indic          laboratori         level            lipid       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.3333   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     manag            profil           provid         recommend     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.3333   Mean   :0.3333   Mean   :0.3333  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:0.5000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     result        triglycerid       PatientID  \n Min.   :0.0000   Min.   :0.0000   Min.   :1.0  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.5  \n Median :0.0000   Median :0.0000   Median :2.0  \n Mean   :0.3333   Mean   :0.3333   Mean   :2.0  \n 3rd Qu.:0.5000   3rd Qu.:0.5000   3rd Qu.:2.5  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.0  \n\n# Check the structure of the data\nstr(dtm_df)\n\n'data.frame':   3 obs. of  35 variables:\n $ bilater      : num  1 0 0\n $ breath       : num  1 0 0\n $ chest        : num  1 0 0\n $ cough        : num  1 0 0\n $ infiltr      : num  1 0 0\n $ patient      : num  1 1 0\n $ persist      : num  1 0 0\n $ present      : num  1 0 0\n $ reveal       : num  1 0 0\n $ short        : num  1 0 0\n $ symptom      : num  1 0 0\n $ xray         : num  1 0 0\n $ antihypertens: num  0 1 0\n $ conduct      : num  0 1 0\n $ diagnos      : num  0 1 0\n $ examin       : num  0 1 0\n $ hypertens    : num  0 1 0\n $ medic        : num  0 1 0\n $ physician    : num  0 1 0\n $ prescrib     : num  0 1 0\n $ thorough     : num  0 1 0\n $ cholesterol  : num  0 0 1\n $ dietari      : num  0 0 1\n $ elev         : num  0 0 1\n $ indic        : num  0 0 1\n $ laboratori   : num  0 0 1\n $ level        : num  0 0 1\n $ lipid        : num  0 0 1\n $ manag        : num  0 0 1\n $ profil       : num  0 0 1\n $ provid       : num  0 0 1\n $ recommend    : num  0 0 1\n $ result       : num  0 0 1\n $ triglycerid  : num  0 0 1\n $ PatientID    : num  1 2 3\n\n# Check for missing values\nsum(is.na(dtm_df))\n\n[1] 0\n\n# Check variable names\ncolnames(dtm_df)\n\n [1] \"bilater\"       \"breath\"        \"chest\"         \"cough\"        \n [5] \"infiltr\"       \"patient\"       \"persist\"       \"present\"      \n [9] \"reveal\"        \"short\"         \"symptom\"       \"xray\"         \n[13] \"antihypertens\" \"conduct\"       \"diagnos\"       \"examin\"       \n[17] \"hypertens\"     \"medic\"         \"physician\"     \"prescrib\"     \n[21] \"thorough\"      \"cholesterol\"   \"dietari\"       \"elev\"         \n[25] \"indic\"         \"laboratori\"    \"level\"         \"lipid\"        \n[29] \"manag\"         \"profil\"        \"provid\"        \"recommend\"    \n[33] \"result\"        \"triglycerid\"   \"PatientID\"    \n\n# Display the first few rows of the data\nhead(dtm_df)\n\n  bilater breath chest cough infiltr patient persist present reveal short\n1       1      1     1     1       1       1       1       1      1     1\n2       0      0     0     0       0       1       0       0      0     0\n3       0      0     0     0       0       0       0       0      0     0\n  symptom xray antihypertens conduct diagnos examin hypertens medic physician\n1       1    1             0       0       0      0         0     0         0\n2       0    0             1       1       1      1         1     1         1\n3       0    0             0       0       0      0         0     0         0\n  prescrib thorough cholesterol dietari elev indic laboratori level lipid manag\n1        0        0           0       0    0     0          0     0     0     0\n2        1        1           0       0    0     0          0     0     0     0\n3        0        0           1       1    1     1          1     1     1     1\n  profil provid recommend result triglycerid PatientID\n1      0      0         0      0           0         1\n2      0      0         0      0           0         2\n3      1      1         1      1           1         3\n\n# Visualize the distribution of PatientID\nggplot(dtm_df, aes(x = PatientID)) +\n  geom_bar(fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Distribution of PatientID\")\n\n\n\n# Visualize the most common terms\nterm_freq &lt;- colSums(dtm_df[, -ncol(dtm_df)])\nterm_freq_df &lt;- data.frame(Term = names(term_freq), Frequency = term_freq)\nterm_freq_df &lt;- term_freq_df[order(-term_freq_df$Frequency), ]\n\n# Plot the top N terms\ntop_n_terms &lt;- 10\nggplot(head(term_freq_df, top_n_terms), aes(x = reorder(Term, Frequency), y = Frequency)) +\n  geom_bar(stat = \"identity\", fill = \"green\", alpha = 0.7) +\n  labs(title = paste(\"Top\", top_n_terms, \"Terms\"), x = \"Term\", y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/d/index.html",
    "href": "posts/d/index.html",
    "title": "Bussines analytics",
    "section": "",
    "text": "In this group project, you will work with analysts’ forecast data of earning per share (EPS) provided by Wharton Research Data Services (WRDS). Institutional Brokers’ Estimate System (I/B/E/S) provides historical data on certain financial indicators collected from thousands of individual analysts working in more than 3,000 broker houses.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nNFLX &lt;- read.csv(\"C:/Users/HP/Downloads/NFLX.csv\", header=TRUE)\nnames(NFLX)\n\n [1] \"TICKER\"      \"CNAME\"       \"ACTDATS\"     \"ESTIMATOR\"   \"ANALYS\"     \n [6] \"FPI\"         \"MEASURE\"     \"VALUE\"       \"FPEDATS\"     \"REVDATS\"    \n[11] \"REVTIMS\"     \"ANNDATS\"     \"ANNTIMS\"     \"ACTUAL\"      \"ANNDATS_ACT\"\n[16] \"ANNTIMS_ACT\""
  },
  {
    "objectID": "posts/c/index.html",
    "href": "posts/c/index.html",
    "title": "Iris exploratory data analysis",
    "section": "",
    "text": "The script employs exploratory data analysis to visualize features of the Iris dataset, subsequently implementing a decision tree model for species classification, evaluating its accuracy on a test set, and presenting a confusion matrix, offering insights into both data exploration and machine learning application on the Iris dataset.\n\n# Load necessary libraries\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n# Load your IRSI data (replace \"your_dataset.csv\" with your actual dataset file)\nIRSI_data &lt;- read.csv(\"C:/Users/HP/Downloads/Iris.csv\")\n\n# Display the first few rows of the dataset\nhead(IRSI_data)\n\n  Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm     Species\n1  1           5.1          3.5           1.4          0.2 Iris-setosa\n2  2           4.9          3.0           1.4          0.2 Iris-setosa\n3  3           4.7          3.2           1.3          0.2 Iris-setosa\n4  4           4.6          3.1           1.5          0.2 Iris-setosa\n5  5           5.0          3.6           1.4          0.2 Iris-setosa\n6  6           5.4          3.9           1.7          0.4 Iris-setosa\n\n# Check the structure of the dataset\nstr(IRSI_data)\n\n'data.frame':   150 obs. of  6 variables:\n $ Id           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ SepalLengthCm: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ SepalWidthCm : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ PetalLengthCm: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ PetalWidthCm : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species      : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\n# Summary statistics for numerical variables\nsummary(IRSI_data)\n\n       Id         SepalLengthCm    SepalWidthCm   PetalLengthCm  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.054   Mean   :3.759  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  PetalWidthCm     Species         \n Min.   :0.100   Length:150        \n 1st Qu.:0.300   Class :character  \n Median :1.300   Mode  :character  \n Mean   :1.199                     \n 3rd Qu.:1.800                     \n Max.   :2.500                     \n\n# Check for missing values\nsapply(IRSI_data, function(x) sum(is.na(x)))\n\n           Id SepalLengthCm  SepalWidthCm PetalLengthCm  PetalWidthCm \n            0             0             0             0             0 \n      Species \n            0 \n\n# Frequency table for the Species variable\ntable(IRSI_data$Species)\n\n\n    Iris-setosa Iris-versicolor  Iris-virginica \n             50              50              50 \n\n# Print column names\ncolnames(IRSI_data)\n\n[1] \"Id\"            \"SepalLengthCm\" \"SepalWidthCm\"  \"PetalLengthCm\"\n[5] \"PetalWidthCm\"  \"Species\"      \n\n# Correlation matrix\ncor(IRSI_data[, 1:4])\n\n                      Id SepalLengthCm SepalWidthCm PetalLengthCm\nId             1.0000000     0.7166763   -0.3977288     0.8827473\nSepalLengthCm  0.7166763     1.0000000   -0.1093692     0.8717542\nSepalWidthCm  -0.3977288    -0.1093692    1.0000000    -0.4205161\nPetalLengthCm  0.8827473     0.8717542   -0.4205161     1.0000000\n\n# Load the corrplot library\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Extract the correlation matrix\ncor_matrix &lt;- cor(IRSI_data[, 2:5])  # Assuming IRSI_data is your data frame\n\n# Create a correlation plot\ncorrplot(cor_matrix, method = \"circle\", tl.col = \"black\", addrect = 2)\n\n\n\n# Boxplot by Species\npar(mfrow = c(1, 4))\nfor (i in 1:4) {\n  boxplot(IRSI_data[, i] ~ IRSI_data$Species, main = colnames(IRSI_data)[i], col = c(\"skyblue\", \"lightgreen\", \"lightcoral\"))\n}\n\n\n\n# Histogram for SepalLengthCm\nhist(IRSI_data$SepalLengthCm, col = \"skyblue\", main = \"Sepal Length Distribution\")\n# Density plot for PetalWidthCm\nggplot(IRSI_data, aes(x = PetalWidthCm, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  ggtitle(\"Petal Width Density by Species\")\n# Violin plot for PetalLengthCm\nggplot(IRSI_data, aes(x = Species, y = PetalLengthCm, fill = Species)) +\n  geom_violin() +\n  ggtitle(\"Petal Length Distribution by Species\")"
  },
  {
    "objectID": "posts/d/index.html#how-to-read-the-data",
    "href": "posts/d/index.html#how-to-read-the-data",
    "title": "Bussines analytics",
    "section": "How to read the data",
    "text": "How to read the data\nThe first row in NFLX data set: On 09‐Aug-2002 (ANNDATS), analyst 6749 (ANALYS) at Estimator 1872 (ESTIMATOR) predicts that the EPS (MEASURE) for NETFLIX INC. (CNAME) with a ticker of NFLX (TICKER) with forecast period ending 30‐Sep-2002 (FPEDATS) is -$0.0086 (VALUE). This estimates was entered into the database on 12‐Aug-2002 (ACTDATS). On 17-Oct-2002 (ANNDATS_ACT), NETFLIX INC. announced an actual EPS of $7e-04 ($0.0007) (ACTUAL) for this quarter (FPI=6).\n\nhead(NFLX,n=1)\n\n  TICKER        CNAME  ACTDATS ESTIMATOR ANALYS FPI MEASURE   VALUE  FPEDATS\n1   NFLX NETFLIX INC. 20020812      1872   6749   6     EPS -0.0086 20020930\n   REVDATS  REVTIMS  ANNDATS  ANNTIMS ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1 20021018 17:02:56 20020809 14:00:00 -7e-04    20021017    17:04:00"
  },
  {
    "objectID": "posts/d/index.html#your-turn",
    "href": "posts/d/index.html#your-turn",
    "title": "Bussines analytics",
    "section": "Your Turn:",
    "text": "Your Turn:\n\n\n\n\n\n\nTask 1A: Calculate Missingness\n\n\n\nCheck to see the missing values in NFLX dataset and calculate the percent missing for each variable in NFLX and list your findings in R object called NFLX_missingness. NFLX_missingness is a dataframe with two columns: The first column, Variable, stores the variable names and the second column, Missingness shows the percent missing in percentage points with two decimal points."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-1a",
    "href": "posts/d/index.html#your-code-for-task-1a",
    "title": "Bussines analytics",
    "section": "Your code for Task 1A",
    "text": "Your code for Task 1A\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n# Read CSV data\nNFLX &lt;- read.csv(\"C:/Users/HP/Downloads/NFLX.csv\", header=TRUE)\n\n# Calculate missing values percentage function\ncalculate_missingness &lt;- function(data) {\n  data %&gt;%\n    summarise_all(~mean(is.na(.)) * 100)\n}\n\n# Calculate missing values\nNFLX_missingness &lt;- calculate_missingness(NFLX)\n\n# Print missing values summary\nprint(NFLX_missingness)\n\n  TICKER CNAME ACTDATS ESTIMATOR ANALYS FPI MEASURE VALUE FPEDATS REVDATS\n1      0     0       0         0      0   0       0     0       0       0\n  REVTIMS ANNDATS ANNTIMS   ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1       0       0       0 4.115304    4.115304           0\n\n# Visualize missing values\nmissingness_plot &lt;- function(data) {\n  data_long &lt;- gather(data, key = \"Variable\", value = \"MissingPercentage\")\n  ggplot(data_long, aes(x = reorder(Variable, -MissingPercentage), y = MissingPercentage)) +\n    geom_bar(stat = \"identity\", fill = \"skyblue\") +\n    labs(title = \"Missing Values Percentage by Variable\", \n         x = \"Variable\", y = \"Missing Percentage\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\n\n# Generate and display missing values plot\nmissingness_plot(NFLX_missingness)\n\n\n\n\n\n\n\n\n\n\nTask 1B: Data Manipulation\n\n\n\nConduct the following data manipulations on NFLX:\n\nDrop rows from the data set when a variable has a missing value\nDrop rows from the data set the quarterly forecasts (drop FPI=6)\nDeclare TICKER, CNAME, ESTIMATOR , ANALYS, FPI , and MEASURE variables as factor\nDeclare ACTDATS, FPEDATS , ANNDATS, REVDATS, ANNDATS_ACT as time variable.\nDrop ANNTIMS_ACT, ANNTIMS , and REVTIMS\nCreate a new column named YEAR that captures the year in FPEDATS\nName your reduced dataset as NFLX1\nPrint out data structure and the summary of NFLX1"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-1b",
    "href": "posts/d/index.html#your-code-for-task-1b",
    "title": "Bussines analytics",
    "section": "Your code for Task 1B",
    "text": "Your code for Task 1B\n\n# Copy NFLX to NFLX1 without assigning data types\nNFLX1 &lt;- NFLX\n\n# Drop rows from the data set when a variable has a missing value\nNFLX1 &lt;- NFLX1 %&gt;% na.omit()\n\n# Drop rows from the data set where FPI=6\nNFLX1 &lt;- NFLX1 %&gt;% filter(FPI != 6)\n\n# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS\nNFLX1 &lt;- NFLX1 %&gt;% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)\n\n# Create a new column named YEAR that is an exact copy of the data in FPEDATS\nNFLX1 &lt;- NFLX1 %&gt;% mutate(YEAR = FPEDATS)\n\n# Print out data structure and the summary of NFLX1\nstr(NFLX1)\n\n'data.frame':   2603 obs. of  14 variables:\n $ TICKER     : chr  \"NFLX\" \"NFLX\" \"NFLX\" \"NFLX\" ...\n $ CNAME      : chr  \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" ...\n $ ACTDATS    : int  20020805 20021202 20021202 20021202 20021205 20030106 20030115 20030116 20030121 20030314 ...\n $ ESTIMATOR  : int  183 2178 1872 220 2178 1872 2227 220 1872 481 ...\n $ ANALYS     : int  79868 80485 6749 57596 80485 6749 82629 57596 6749 81599 ...\n $ FPI        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ MEASURE    : chr  \"EPS\" \"EPS\" \"EPS\" \"EPS\" ...\n $ VALUE      : num  -0.025 -0.0321 -0.0207 -0.0179 -0.0286 -0.0136 -0.0164 -0.0071 0.0107 0.0129 ...\n $ FPEDATS    : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n $ REVDATS    : int  20021129 20021202 20021202 20021206 20021205 20030114 20030115 20030417 20030402 20030409 ...\n $ ANNDATS    : int  20020805 20021202 20021202 20021202 20021204 20030102 20030115 20030116 20030116 20030314 ...\n $ ACTUAL     : num  -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 0.0393 0.0393 0.0393 ...\n $ ANNDATS_ACT: int  20030115 20030115 20030115 20030115 20030115 20030115 20030115 20040121 20040121 20040121 ...\n $ YEAR       : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:217] 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 ...\n  ..- attr(*, \"names\")= chr [1:217] \"5057\" \"5058\" \"5059\" \"5060\" ...\n\nsummary(NFLX1)\n\n    TICKER             CNAME              ACTDATS           ESTIMATOR   \n Length:2603        Length:2603        Min.   :20020805   Min.   :  11  \n Class :character   Class :character   1st Qu.:20101021   1st Qu.: 192  \n Mode  :character   Mode  :character   Median :20141009   Median : 899  \n                                       Mean   :20136831   Mean   :1376  \n                                       3rd Qu.:20180122   3rd Qu.:2502  \n                                       Max.   :20210119   Max.   :4439  \n     ANALYS            FPI      MEASURE              VALUE       \n Min.   :  1047   Min.   :1   Length:2603        Min.   :-0.150  \n 1st Qu.: 71755   1st Qu.:1   Class :character   1st Qu.: 0.190  \n Median : 82010   Median :1   Mode  :character   Median : 0.430  \n Mean   : 89534   Mean   :1                      Mean   : 1.339  \n 3rd Qu.:114459   3rd Qu.:1                      3rd Qu.: 2.015  \n Max.   :194536   Max.   :1                      Max.   : 7.670  \n    FPEDATS            REVDATS            ANNDATS             ACTUAL      \n Min.   :20021231   Min.   :20021129   Min.   :20020805   Min.   :-0.005  \n 1st Qu.:20101231   1st Qu.:20110120   1st Qu.:20101021   1st Qu.: 0.250  \n Median :20141231   Median :20141013   Median :20141009   Median : 0.430  \n Mean   :20137082   Mean   :20137740   Mean   :20136830   Mean   : 1.384  \n 3rd Qu.:20181231   3rd Qu.:20180122   3rd Qu.:20180122   3rd Qu.: 2.680  \n Max.   :20201231   Max.   :20210119   Max.   :20210119   Max.   : 6.080  \n  ANNDATS_ACT            YEAR         \n Min.   :20030115   Min.   :20021231  \n 1st Qu.:20110126   1st Qu.:20101231  \n Median :20150120   Median :20141231  \n Mean   :20145973   Mean   :20137082  \n 3rd Qu.:20190117   3rd Qu.:20181231  \n Max.   :20210119   Max.   :20201231  \n\n\n\n\n\n\n\n\nTask 2: Calculate Number of Analysts and Brokerage Houses\n\n\n\n\nCalculate the total number of unique analysts in NFLX1 dataset that provide forecasts each year and name your R object as NumberAnalyst\nCalculate the total number of unique brokerage houses (ESTIMATOR) in NFLX1 dataset that provide forecasts each year and name your R object as NumberBrokerage\nNeed Written Response in this callout: In which year(s) we have the highest number of unique analysts providing forecasts for NFLX ticker? In which year(s), we have the highest number of unique brokerage houses providing forecasts for the NFLX ticker.\n\n\n\n\n\n\n\nThe year 2020"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-2",
    "href": "posts/d/index.html#your-code-for-task-2",
    "title": "Bussines analytics",
    "section": "Your code for Task 2",
    "text": "Your code for Task 2\n\n# Create a new column named YEAR that captures the year in FPEDATS in the specified format\nNFLX1 &lt;- NFLX1 %&gt;%\n  mutate(YEAR = format(FPEDATS))\n\n# Calculate the total number of unique analysts providing forecasts each year\nNumberAnalyst &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  distinct(ANALYS) %&gt;%\n  summarise(NumAnalysts = n_distinct(ANALYS))\n\n# Print the NumberAnalyst object\nprint(NumberAnalyst)\n\n# A tibble: 19 × 2\n   YEAR     NumAnalysts\n   &lt;chr&gt;          &lt;int&gt;\n 1 20021231           5\n 2 20031231           9\n 3 20041231          19\n 4 20051231          17\n 5 20061231          20\n 6 20071231          20\n 7 20081231          20\n 8 20091231          33\n 9 20101231          37\n10 20111231          40\n11 20121231          38\n12 20131231          42\n13 20141231          45\n14 20151231          47\n15 20161231          46\n16 20171231          48\n17 20181231          56\n18 20191231          46\n19 20201231          49\n\n# Calculate the total number of unique brokerage houses providing forecasts each year\nNumberBrokerage &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  distinct(ESTIMATOR) %&gt;%\n  summarise(NumBrokerage = n_distinct(ESTIMATOR))\n\n# Print the NumberBrokerage object\nprint(NumberBrokerage)\n\n# A tibble: 19 × 2\n   YEAR     NumBrokerage\n   &lt;chr&gt;           &lt;int&gt;\n 1 20021231            5\n 2 20031231            8\n 3 20041231           18\n 4 20051231           17\n 5 20061231           19\n 6 20071231           18\n 7 20081231           21\n 8 20091231           32\n 9 20101231           38\n10 20111231           35\n11 20121231           36\n12 20131231           43\n13 20141231           40\n14 20151231           46\n15 20161231           45\n16 20171231           49\n17 20181231           54\n18 20191231           43\n19 20201231           44\n\n\n\n\n\n\n\n\nTask 3: Get the most recent forecast in each year\n\n\n\n\nIt is quite possible that an analyst makes multiple forecasts throughout the year for the same fiscal period. Remove observations from NFLX1 if an analyst has multiple predictions for the same year and keep the last one (the most recent forecast for each year). Name your new dataset as NFLX2. This step is crucial for successful execution of the following tasks. Print the dimension of NFLX2.\nCheck your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-3",
    "href": "posts/d/index.html#your-code-for-task-3",
    "title": "Bussines analytics",
    "section": "Your code for Task 3",
    "text": "Your code for Task 3\n\n# Enter your code for Task 3 below \n# Get the most recent forecast for each analyst in each year\nNFLX2 &lt;- NFLX1 %&gt;%\n  group_by(ANALYS, YEAR) %&gt;%\n  filter(REVDATS == max(REVDATS)) %&gt;%\n  ungroup()\n\n# Print the dimension of NFLX2\nprint(dim(NFLX2))\n\n[1] 641  14\n\n# Check your work\n# If NFLX2 has 641 rows and 14 columns, you are on the right track.\n# If not, please seek help!\n\n\n\n\n\n\n\n\n\nTask 4: Calculate past accuracy\n\n\n\n\nCreate a copy of NFLX2 and call it NFLX3\nFor every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy. In the calculation of forecast performance, you can use the VALUE-ACTUAL as the forecast accuracy measure.\nFor each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nAs an example, consider the year 2006, where analyst 1047, employed at brokerage house 464, provided an estimated end-of-period EPS of 0.0929 (VALUE). However, the actual EPS for that year turned out to be 0.1014 (ACTUAL), resulting in a forecasting error of -0.0085. Consequently, in the subsequent year, 2007, the past_accuracy metric for analyst 1047 would reflect this error by taking the value of -0.0085 (VALUE-ACTUAL).\nThis action will create some missing values and this is perfectly fine.\nIf your code produces 144 NAs, then you are on the right track.\nNote that we are creating copies of the original dataset at each step to facilitate error detection in case any mistakes occur during the process."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-4",
    "href": "posts/d/index.html#your-code-for-task-4",
    "title": "Bussines analytics",
    "section": "Your code for Task 4",
    "text": "Your code for Task 4\n\n# Create a copy of NFLX2 and call it NFLX3\nNFLX3 &lt;- NFLX2\n\n# Task 4: Calculate past accuracy\n\n# For every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy.\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ANALYS) %&gt;%\n  mutate(accuracy = VALUE - ACTUAL)\n\n# For each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  arrange(YEAR) %&gt;%\n  mutate(past_accuracy = lag(accuracy))\n\n# Check if the code produces 144 NAs\nsum(is.na(NFLX3$past_accuracy))\n\n[1] 144\n\n\n\n\n\n\n\n\n\n\nTask 5: Forecast Horizon\n\n\n\n\nThe longer the forecast horizon, the higher the uncertainty associated with EPS forecasts. To control for this fact, create a new column in NFLX3 called horizon that captures the forecast horizon (ANNDATS_ACT- ANNDATS) for each analyst.\nWe anticipate observing a negative correlation between accuracy and horizon. Typically, as the forecast horizon increases, the accuracy tends to decrease, and vice versa. However, in our dataset, there is an exception where we find a positive correlation between accuracy and horizon for one specific year. Write an R code to identify and determine which year exhibits this positive correlation.\nNeed Written Response in this callout: Enter the year in here.\n\n\n\n\n\n\n\nThe year with the correlation was 2018 2012 2011 2013 2015 with 2018 having the highest correlation of 0.24300105."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-5",
    "href": "posts/d/index.html#your-code-for-task-5",
    "title": "Bussines analytics",
    "section": "Your code for Task 5",
    "text": "Your code for Task 5\n\n# Enter your code for Task 5 below\n# Calculate forecast horizon\nNFLX3 &lt;- NFLX3 %&gt;%\n  mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = \"days\")))\n\n# Calculate correlation between accuracy and horizon\ncorrelation_by_year &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(correlation = cor(accuracy, horizon, use = \"complete.obs\"))\n\n# Convert YEAR to POSIXlt format\ncorrelation_by_year$YEAR &lt;- as.POSIXlt(correlation_by_year$YEAR, format = \"%Y\")\ncorrelation_by_year$YEAR &lt;- format(correlation_by_year$YEAR, \"%Y-%m-%d %H:%M:%S\")\n\n# Find positive correlation year\npositive_corr_year &lt;- correlation_by_year %&gt;%\n  filter(correlation &gt; 0)\n\n# Print positive correlation year with correlation values\nprint(positive_corr_year)\n\n# A tibble: 5 × 2\n  YEAR                correlation\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 2011-12-07 00:00:00      0.227 \n2 2012-12-07 00:00:00      0.0654\n3 2013-12-07 00:00:00      0.0214\n4 2015-12-07 00:00:00      0.0558\n5 2018-12-07 00:00:00      0.243 \n\n# If you want to print only the YEAR values without correlation values\n# print(positive_corr_year$YEAR)\n\n\n\n\n\n\n\n\n\nTable 6: Experience\n\n\n\n\nWe assume that if an analyst is monitoring a company for a long period of time, he/she is expected to make more informed predictions. Create a new column in NFLX3 called experience that counts the cumulative number of years the analyst monitor (have predictions) the company. Print the summary of experience column.\nHint: Try to use cumsum() function in R.\nNeed Written Response in this callout: Which analyst (s) has the highest number of experience in NFLX3 dataset and for how long do they monitor the NFLX ticker?\n\n\n\n\n\n\n\nBased on the analaysis conducted on the NFLX3 data there were two analysts of unique identifier 72088 and 77748 with the highest experience who observed the NTFLX ticker for 17 years each."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-6",
    "href": "posts/d/index.html#your-code-for-task-6",
    "title": "Bussines analytics",
    "section": "Your code for Task 6",
    "text": "Your code for Task 6\n\n# Enter your code for Task 6 below \n# Calculate cumulative experience\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  mutate(experience = cumsum(!duplicated(YEAR)))\n\n# Find max experience analysts\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience))\n\n# Summary of experience column\nsummary(NFLX3$experience)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.192   6.000  17.000 \n\n# Analyst(s) with highest experience\nmax_experience\n\n# A tibble: 2 × 2\n  ANALYS experience\n   &lt;int&gt;      &lt;int&gt;\n1  72088         17\n2  77748         17\n\n\n\n\n\n\n\n\n\n\nTask 7: Size\n\n\n\n\nIf a brokerage house has multiple analysts providing predictions for the same company, it may indicate a greater allocation of resources for company analysis. To capture this, create a new column in the NFLX3 dataset called size that calculates the total count of unique analysts employed per year by each brokerage house (ESTIMATOR)\nNeed Written Response in this callout: Print the frequencies for size variable. What does this frequency table reveal about the distribution of the number of analysts hired by brokerage houses in this dataset?\n\n\n\n\n\n\nThe data analysis indicates a notable decline in hiring frequency with an increase in the number of analyst. This trend suggests a strong inclination towards employing a single analyst per season within a brokerage."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-7",
    "href": "posts/d/index.html#your-code-for-task-7",
    "title": "General R knowledge",
    "section": "",
    "text": "# Enter your code for Task 7 below \nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ESTIMATOR) %&gt;%\n  mutate(size = n_distinct(ANALYS))\n\nsize_freq &lt;- table(NFLX3$size)\n\nsize_table &lt;- as.data.frame(size_freq)\ncolnames(size_table) &lt;- c(\"Number of Analysts\", \"Frequency\")\n\nsize_table &lt;- size_table[order(-size_table$Frequency), ]\n\nsize_table %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\nNumber of Analysts\nFrequency\n\n\n\n\n1\n560\n\n\n2\n72\n\n\n3\n9\n\n\n\n\n\n\nsummary_stats &lt;- summary(NFLX3$size)\n\nsummary_data &lt;- data.frame(\n  Statistic = names(summary_stats),\n  Value = as.character(summary_stats)\n)\n\nsummary_data %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\nStatistic\nValue\n\n\n\n\nMin.\n1\n\n\n1st Qu.\n1\n\n\nMedian\n1\n\n\nMean\n1.14040561622465\n\n\n3rd Qu.\n1\n\n\nMax.\n3\n\n\n\n\n\n\nggplot(size_table, aes(x = factor(`Number of Analysts`), y = Frequency, fill = `Number of Analysts`)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Number of Unique Analysts per Year and ESTIMATOR\") +\n  xlab(\"Number of Analysts\") +\n  ylab(\"Frequency\") +\n  scale_fill_manual(\n    values = c(\"#1f78b4\", \"#33a02c\", \"#e31a1c\", \"#ff7f00\", \"#6a3d9a\")  # Example colors, feel free to change\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nTask 8: Prediction 1\n\n\n\n\nIn the year 2020, NETFLIX reported an actual earnings per share (EPS) of $6.08. To predict this EPS value based on historical data, we will employ a linear regression model using the dataset NFLX3 up until the year 2019. In this model, the target variable will be ACTUAL and the predictor variables will include VALUE and past_accuracy. C.all your model as model1.\nNeed Written Response in this callout: Using the linear regression model ‘model1,’ which has been trained on historical data up to the year 2019, what is the forecasted EPS (Earnings Per Share) for the year 2020? Please provide a brief explanation of the method you employed to make this prediction. If you encountered any challenges or were unable to make the calculation, briefly describe the specific issues you encountered.\n\n\n\n\n\n\n\nWhen utilizing the linear regression model ‘model1’, trained on historical data up until 2019, to forecast EPS for 2020, a rigorous evaluation of the model’s fit is conducted using the R-squared value. Should this metric surpass the 0.5 threshold, indicating a robust fit, the forecasting process commences. Central to this process is the calculation of the mean of the ‘past_accuracy’ variable. In instances where the R-squared value meets the prescribed criterion, a new dataset is meticulously curated, ensuring the incorporation of pertinent independent variables. Subsequently, EPS estimates for the future period are meticulously derived through the adept utilization of the ‘predict’ function.However, in scenarios where the R-squared value fails to meet the specified threshold, a cautionary warning emerges, signaling potential inaccuracies in the subsequent EPS predictions. Addressing specific nuances in data configuration or refining the model’s training approach becomes imperative, safeguarding the accuracy and reliability of future EPS projections."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-8",
    "href": "posts/d/index.html#your-code-for-task-8",
    "title": "Bussines analytics",
    "section": "Your code for Task 8",
    "text": "Your code for Task 8\n\n# Calculate mean of past_accuracy\nmean_pa &lt;- mean(NFLX3$past_accuracy, na.rm = TRUE)\n\n\n# Create linear regression model\nmodel &lt;- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)\n\n# Get R-squared value\nr_squared &lt;- summary(model)$r.squared\n\n# Check R-squared for forecast\nif (r_squared &gt; 0.5) {\n  # Future data\n  new_data &lt;- data.frame(VALUE = 6.08, past_accuracy = mean_pa)\n  \n  # Predict EPS for future\n  pred_eps &lt;- predict(model, newdata = new_data)\n  \n  # Print forecasted EPS\n  cat(\"Forecasted EPS for future period: $\", round(pred_eps, 2))\n} else {\n  # Print warning for low R-squared\n  cat(\"R-squared is low; model may not predict accurately.\")\n}\n\nForecasted EPS for future period: $ 6.3\n\n# Print mean past_accuracy\ncat(\"Mean past_accuracy: \", round(mean_pa, 2))\n\nMean past_accuracy:  -0.09\n\n\n\n\n\n\n\n\n\n\nTask 9: Prediction 2\n\n\n\n\nAs an alternative approach, instead of modeling the ‘ACTUAL’ value, we can obtain the mean and median forecasts for the year 2020 as our best estimates of the EPS value for that year.\nNeed Written Response in this callout: Please calculate these forecasts and then compare them with the results from the previous task. Finally, provide your insights and comments based on your findings.\n\n\n\n\n\n\n\nIn the alternative approach for predicting Earnings Per Share (EPS) in 2020, mean and median forecasts were used, yielding values of approximately $1.24 and $0.41, respectively. While the mean and median forecasts provide straightforward averages and midpoints, the model-based forecast from the earlier linear regression model offers a more detailed and potentially precise prediction by considering historical relationships and variables like ‘past_accuracy.’ The reliability of the model-based forecast depends on the quality of its fit, indicated by the R-squared value. Despite their simplicity, mean and median forecasts lack the predictive power of a well-fitted model and might miss data nuances. The choice between these methods should consider data quality and specific analysis context. The model-driven forecast, although complex, offers a sophisticated approach, while mean and median forecasts are valuable alternatives, especially in uncertain model fits or when simpler predictions suffice. Consideration of factors such as context, data quality, and desired precision guides the selection of the appropriate forecasting method."
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task-9",
    "href": "posts/d/index.html#your-code-for-task-9",
    "title": "Bussines analytics",
    "section": "Your code for Task 9",
    "text": "Your code for Task 9\n\n# Calculate mean and median forecasts\nmean_forecast &lt;- mean(NFLX3$VALUE, na.rm = TRUE)\nmedian_forecast &lt;- median(NFLX3$VALUE, na.rm = TRUE)\n\n# Print mean and median forecasts\ncat(\"Mean forecast 2020: $\", round(mean_forecast, 2))\n\nMean forecast 2020: $ 1.24\n\ncat(\"Median forecast 2020: $\", round(median_forecast, 2))\n\nMedian forecast 2020: $ 0.41\n\n\n\n\n\n\n\n\n\n\nTask 10: Averages\n\n\n\n\nGenerate a new dataset named NFLX4 by aggregating data from NFLX3 Include the variables size, experience, horizon, accuracy, past_accuracy, and ACTUAL in NFLX4. When calculating the yearly averages for these variables, ignore any missing values (NAs). Present a summary of the NFLX4 dataset.\nNeed Written Response in this callout: Subsequently, employ correlation analysis or exploratory data analysis to get insights into the relationships between these variables and ‘ACTUAL,’ if such relationships exist.\n\n\n\n\n\n\n\nActual EPS exhibits a positive correlation with analyst experience, indicating more accurate predictions with longer monitoring periods. Conversely, negative correlations with horizon, accuracy, and past accuracy imply reduced accuracy with longer forecast horizons and lower past accuracy. Multiple analysts in brokerage houses show varied impacts on predictions, underscoring the importance of resource allocation for comprehensive analysis.\n\n\n\n\n\n\n# Aggregate data and calculate yearly averages\nNFLX4 &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    size = mean(size, na.rm = TRUE),\n    experience = mean(experience, na.rm = TRUE),\n    horizon = mean(horizon, na.rm = TRUE),\n    accuracy = mean(accuracy, na.rm = TRUE),\n    past_accuracy = mean(past_accuracy, na.rm = TRUE),\n    ACTUAL = mean(ACTUAL, na.rm = TRUE)\n  )\n\n# Summary of NFLX4 dataset\nsummary(NFLX4)\n\n     YEAR                size         experience       horizon       \n Length:19          Min.   :1.000   Min.   :1.000   Min.   :0.06284  \n Class :character   1st Qu.:1.074   1st Qu.:2.664   1st Qu.:0.08547  \n Mode  :character   Median :1.105   Median :3.400   Median :0.09289  \n                    Mean   :1.132   Mean   :3.611   Mean   :0.09004  \n                    3rd Qu.:1.202   3rd Qu.:4.869   3rd Qu.:0.09512  \n                    Max.   :1.300   Max.   :6.061   Max.   :0.10656  \n                                                                     \n    accuracy         past_accuracy           ACTUAL       \n Min.   :-0.822085   Min.   :-0.798219   Min.   :-0.0050  \n 1st Qu.:-0.019087   1st Qu.:-0.028736   1st Qu.: 0.0914  \n Median :-0.015035   Median :-0.013423   Median : 0.2643  \n Mean   :-0.048310   Mean   :-0.060652   Mean   : 0.9248  \n 3rd Qu.:-0.005415   3rd Qu.:-0.009260   3rd Qu.: 0.5678  \n Max.   : 0.121449   Max.   :-0.001547   Max.   : 6.0800  \n                     NA's   :1                            \n\n# Correlation analysis\ncorrelation_matrix &lt;- cor(NFLX4[, c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\")], use = \"complete.obs\")\n\n# Print correlation matrix\nprint(correlation_matrix)\n\n                     size  experience    horizon    accuracy past_accuracy\nsize           1.00000000  0.07451284 -0.1317823 -0.04537307    -0.1810330\nexperience     0.07451284  1.00000000 -0.4844637 -0.25882136    -0.4620906\nhorizon       -0.13178225 -0.48446371  1.0000000  0.22264895     0.4979377\naccuracy      -0.04537307 -0.25882136  0.2226489  1.00000000    -0.1604379\npast_accuracy -0.18103301 -0.46209061  0.4979377 -0.16043792     1.0000000\nACTUAL         0.18223220  0.68707354 -0.6346966 -0.31928984    -0.7958850\n                  ACTUAL\nsize           0.1822322\nexperience     0.6870735\nhorizon       -0.6346966\naccuracy      -0.3192898\npast_accuracy -0.7958850\nACTUAL         1.0000000\n\n# Create a correlogram\ncorrplot(correlation_matrix, method = \"color\", type = \"upper\")"
  },
  {
    "objectID": "posts/d/index.html#your-code-for-task",
    "href": "posts/d/index.html#your-code-for-task",
    "title": "Bussines analytics",
    "section": "Your code for Task",
    "text": "Your code for Task\n\n# Enter your code for Task 7 below \n# Count unique analysts per year and brokerage house (ESTIMATOR)\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ESTIMATOR) %&gt;%\n  mutate(size = n_distinct(ANALYS))\n\n# Print the frequencies for the size variable\nsize_freq &lt;- table(NFLX3$size)\nprint(size_freq)\n\n\n  1   2   3 \n560  72   9 \n\n# Create a frequency table for better visualization\nsize_table &lt;- as.data.frame(size_freq)\ncolnames(size_table) &lt;- c(\"Number of Analysts\", \"Frequency\")\n\n# Sort the table by frequency in descending order\nsize_table &lt;- size_table[order(-size_table$Frequency), ]\n\n# Print the sorted frequency table\nprint(size_table)\n\n  Number of Analysts Frequency\n1                  1       560\n2                  2        72\n3                  3         9\n\n# Summary statistics for size variable\nsummary(NFLX3$size)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    1.00    1.14    1.00    3.00 \n\n# Assuming you want a colorful plot, using a different color palette\nggplot(size_table, aes(x = factor(`Number of Analysts`), y = Frequency, fill = factor(`Number of Analysts`))) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Number of Unique Analysts per Year and ESTIMATOR\") +\n  xlab(\"Number of Analysts\") +\n  ylab(\"Frequency\") +\n  scale_fill_brewer(palette = \"Set3\")  \n\n\n\n\n\n\n\n\n\n\n\n\nTask 8: Prediction 1\n\n\n\n\nIn the year 2020, NETFLIX reported an actual earnings per share (EPS) of $6.08. To predict this EPS value based on historical data, we will employ a linear regression model using the dataset NFLX3 up until the year 2019. In this model, the target variable will be ACTUAL and the predictor variables will include VALUE and past_accuracy. C.all your model as model1.\nNeed Written Response in this callout: Using the linear regression model ‘model1,’ which has been trained on historical data up to the year 2019, what is the forecasted EPS (Earnings Per Share) for the year 2020? Please provide a brief explanation of the method you employed to make this prediction. If you encountered any challenges or were unable to make the calculation, briefly describe the specific issues you encountered.\n\n\n\n\n\n\n\nThe forecasting process involved using ‘model1,’ trained on historical data until 2019, to predict the Earnings Per Share (EPS) for 2020. To assess the model’s accuracy, we calculated the R-squared value, a measure of how well the model fits the data. If the R-squared value was above 0.5, indicating a good fit, we proceeded with the forecast. In such cases, we computed the mean of the ‘past_accuracy’ variable and created a new data frame with specific values for ‘VALUE’ and ‘past_accuracy.’ Using the ‘predict’ function, we estimated the EPS for the future period. However, if the R-squared value was below 0.5, indicating a poor fit, a warning message was issued. This signaled that the model might not reliably predict future EPS values. It was crucial to address any data configuration or model training issues to improve prediction accuracy and ensure the reliability of the forecasting method."
  },
  {
    "objectID": "posts/final/index.html",
    "href": "posts/final/index.html",
    "title": "Comprehensive demonstartion of R prgramming language through a randomly generated dataset.",
    "section": "",
    "text": "We are going to explore R programming language as skill attained over time as i have learnt and understood it. In this project an going to generate a random data set of 100 multiple and different labeling and in turn conduct a simple data arrangement sorting cleaning EDA (exploratory data analysis) and a linear regression model with proper visualization and accuracy measures telemeters."
  },
  {
    "objectID": "posts/final/index.html#introduction",
    "href": "posts/final/index.html#introduction",
    "title": "Comprehensive demonstartion of R prgramming language through a randomly generated dataset.",
    "section": "",
    "text": "We are going to explore R programming language as skill attained over time as i have learnt and understood it. In this project an going to generate a random data set of 100 multiple and different labeling and in turn conduct a simple data arrangement sorting cleaning EDA (exploratory data analysis) and a linear regression model with proper visualization and accuracy measures telemeters."
  },
  {
    "objectID": "posts/final/index.html#data-exploration",
    "href": "posts/final/index.html#data-exploration",
    "title": "Comprehensive demonstartion of R prgramming language through a randomly generated dataset.",
    "section": "DATA EXPLORATION",
    "text": "DATA EXPLORATION\nDue to the size we will just conduct a few exploratory analysis without going into detail but we will demonstrate the relationship between the product bought and the type of people whom bought it and their education level.\n\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Summary statistics by Education Level for Product A and Product B\nsummary_stats &lt;- data %&gt;%\n  group_by(Education_Level) %&gt;%\n  summarise(\n    Mean_Product_A = mean(Product_A),\n    Mean_Product_B = mean(Product_B),\n    Median_Product_A = median(Product_A),\n    Median_Product_B = median(Product_B),\n    Total_Product_A = sum(Product_A),\n    Total_Product_B = sum(Product_B)\n  )\n\nprint(summary_stats)\n\n# A tibble: 4 × 7\n  Education_Level Mean_Product_A Mean_Product_B Median_Product_A\n  &lt;chr&gt;                    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1 Bachelor's               10.3            14.4             11  \n2 High School               9.5            14.5              9.5\n3 Master's                 11.1            16.4             11  \n4 PhD                       9.31           15.5             10  \n# ℹ 3 more variables: Median_Product_B &lt;dbl&gt;, Total_Product_A &lt;int&gt;,\n#   Total_Product_B &lt;int&gt;\n\n\n\nlibrary(ggplot2)\n\n# Visualizing Product A and Product B purchases by Education Level\npurchase_plot &lt;- data %&gt;%\n  gather(Product, Purchases, Product_A, Product_B) %&gt;%\n  ggplot(aes(x = Education_Level, y = Purchases, fill = Product)) +\n  geom_boxplot() +\n  labs(\n    title = \"Product A and Product B Purchases by Education Level\",\n    x = \"Education Level\",\n    y = \"Number of Purchases\"\n  ) +\n  scale_fill_manual(values = c(\"Product_A\" = \"blue\", \"Product_B\" = \"green\")) +\n  theme_minimal()\n\nprint(purchase_plot)"
  },
  {
    "objectID": "posts/final/index.html#logistics-linear-regression-model",
    "href": "posts/final/index.html#logistics-linear-regression-model",
    "title": "Comprehensive demonstartion of R prgramming language through a randomly generated dataset.",
    "section": "LOGISTICS LINEAR REGRESSION MODEL",
    "text": "LOGISTICS LINEAR REGRESSION MODEL\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(kableExtra)\nlibrary(caret)\n\nLoading required package: lattice\n\nset.seed(123)\n\n# Assuming 'Income' is the target variable, creating predictors (X) and target variable (y)\nX &lt;- data %&gt;% select(-ID, -Income)\ny &lt;- data$Income\n\n# Splitting the data into training and testing sets (80% training, 20% testing)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n\n\n# Load the necessary library\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# Fit a linear regression model\nlm_model &lt;- lm(Income ~ ., data = train_data)\n\n# Summary of the model\nsummary(lm_model)\n\n\nCall:\nlm(formula = Income ~ ., data = train_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-26873 -10378     25   8158  38371 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                66128.71   11763.16   5.622 3.84e-07 ***\nID                            -8.06      63.92  -0.126   0.9000    \nAge                          -83.52     159.98  -0.522   0.6033    \nGenderMale                  8207.19    3662.38   2.241   0.0283 *  \nEducation_LevelHigh School -5147.42    5075.71  -1.014   0.3141    \nEducation_LevelMaster's     5432.97    5946.04   0.914   0.3641    \nEducation_LevelPhD         -1400.37    4793.47  -0.292   0.7711    \nRegionNorth                -8155.90    5624.61  -1.450   0.1516    \nRegionSouth                -4549.72    4943.93  -0.920   0.3607    \nRegionWest                 -1352.63    4684.63  -0.289   0.7737    \nProduct_A                   -295.84     657.16  -0.450   0.6540    \nProduct_B                   -626.64     510.97  -1.226   0.2243    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15060 on 68 degrees of freedom\nMultiple R-squared:  0.1438,    Adjusted R-squared:  0.005288 \nF-statistic: 1.038 on 11 and 68 DF,  p-value: 0.4237\n\n\n\n# Making predictions on the test set\npredictions &lt;- predict(lm_model, newdata = test_data)\n\n# Calculate model performance metrics (e.g., RMSE, R-squared)\nrmse &lt;- sqrt(mean((test_data$Income - predictions)^2))\nrsquared &lt;- summary(lm_model)$r.squared\n\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 18523.96 \n\ncat(\"R-squared:\", rsquared, \"\\n\")\n\nR-squared: 0.1437925 \n\n\n\n# Plotting actual vs predicted values\nlibrary(ggplot2)\n\n# Creating a data frame with actual and predicted values\nresults &lt;- data.frame(Actual = test_data$Income, Predicted = predictions)\n\n# Scatterplot\nggplot(results, aes(x = Actual, y = Predicted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Actual vs Predicted Income\", x = \"Actual Income\", y = \"Predicted Income\")\n\n\n\n# Diagnostic plots for linear regression\nplot(lm_model)"
  },
  {
    "objectID": "posts/final/index.html#results-and-discussions",
    "href": "posts/final/index.html#results-and-discussions",
    "title": "Comprehensive demonstartion of R prgramming language through a randomly generated dataset.",
    "section": "RESULTS AND DISCUSSIONS",
    "text": "RESULTS AND DISCUSSIONS\nThe regression analysis aimed to examine the factors influencing the dependent variable, presumably a monetary measure. The model included various independent variables such as ID, Age, Gender, Education Level, Region, and Product type. The coefficients of the model indicate the estimated impact of each variable on the outcome. The results indicate that most coefficients lack statistical significance in predicting the dependent variable, as evident from their high p-values. Notably, Gender (specifically being Male) exhibited a statistically significant positive relationship with the outcome, suggesting that males tend to have a higher value in the dependent variable compared to females. However, caution is advised in interpreting this result due to the limited significance of other variables in the model. Moreover, variables such as Education Level, Region, and Product type did not show substantial influence on the dependent variable, as their coefficients were not statistically significant. The overall model fit is weak, with an adjusted R-squared of 0.005288, indicating that the independent variables collectively explain only a small proportion of the variability in the dependent variable."
  },
  {
    "objectID": "posts/final/index.html#conclusion",
    "href": "posts/final/index.html#conclusion",
    "title": "Comprehensive demonstartion of R prgramming language through a randomly generated dataset.",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nIn conclusion, based on this analysis, the model does not adequately explain the variations in the dependent variable using the selected independent variables. Further investigation or refinement of the model by considering additional relevant factors or improving the dataset might be necessary to achieve a more accurate understanding of the factors affecting the outcome variable."
  }
]